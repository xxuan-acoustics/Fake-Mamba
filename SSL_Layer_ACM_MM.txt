Printing the layers of the pretrained model:
Layer name: , Module: Wav2Vec2Model(
  (feature_extractor): ConvFeatureExtractionModel(
    (conv_layers): ModuleList(
      (0): Sequential(
        (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,))
        (1): Dropout(p=0.0, inplace=False)
        (2): Sequential(
          (0): TransposeLast()
          (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (2): TransposeLast()
        )
        (3): GELU(approximate=none)
      )
      (1): Sequential(
        (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,))
        (1): Dropout(p=0.0, inplace=False)
        (2): Sequential(
          (0): TransposeLast()
          (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (2): TransposeLast()
        )
        (3): GELU(approximate=none)
      )
      (2): Sequential(
        (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,))
        (1): Dropout(p=0.0, inplace=False)
        (2): Sequential(
          (0): TransposeLast()
          (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (2): TransposeLast()
        )
        (3): GELU(approximate=none)
      )
      (3): Sequential(
        (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,))
        (1): Dropout(p=0.0, inplace=False)
        (2): Sequential(
          (0): TransposeLast()
          (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (2): TransposeLast()
        )
        (3): GELU(approximate=none)
      )
      (4): Sequential(
        (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,))
        (1): Dropout(p=0.0, inplace=False)
        (2): Sequential(
          (0): TransposeLast()
          (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (2): TransposeLast()
        )
        (3): GELU(approximate=none)
      )
      (5): Sequential(
        (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,))
        (1): Dropout(p=0.0, inplace=False)
        (2): Sequential(
          (0): TransposeLast()
          (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (2): TransposeLast()
        )
        (3): GELU(approximate=none)
      )
      (6): Sequential(
        (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,))
        (1): Dropout(p=0.0, inplace=False)
        (2): Sequential(
          (0): TransposeLast()
          (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (2): TransposeLast()
        )
        (3): GELU(approximate=none)
      )
    )
  )
  (post_extract_proj): Linear(in_features=512, out_features=1024, bias=True)
  (dropout_input): Dropout(p=0.0, inplace=False)
  (dropout_features): Dropout(p=0.0, inplace=False)
  (quantizer): GumbelVectorQuantizer(
    (weight_proj): Linear(in_features=512, out_features=640, bias=True)
  )
  (project_q): Linear(in_features=768, out_features=768, bias=True)
  (encoder): TransformerEncoder(
    (pos_conv): Sequential(
      (0): Conv1d(1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
      (1): SamePad()
      (2): GELU(approximate=none)
    )
    (layers): ModuleList(
      (0): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.0, inplace=False)
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.0, inplace=False)
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.0, inplace=False)
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.0, inplace=False)
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.0, inplace=False)
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.0, inplace=False)
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (6): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.0, inplace=False)
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (7): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.0, inplace=False)
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (8): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.0, inplace=False)
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (9): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.0, inplace=False)
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (10): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.0, inplace=False)
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (11): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.0, inplace=False)
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (12): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.0, inplace=False)
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (13): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.0, inplace=False)
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (14): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.0, inplace=False)
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (15): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.0, inplace=False)
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (16): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.0, inplace=False)
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (17): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.0, inplace=False)
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (18): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.0, inplace=False)
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (19): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.0, inplace=False)
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (20): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.0, inplace=False)
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (21): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.0, inplace=False)
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (22): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.0, inplace=False)
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (23): TransformerSentenceEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (dropout1): Dropout(p=0.0, inplace=False)
        (dropout2): Dropout(p=0.0, inplace=False)
        (dropout3): Dropout(p=0.0, inplace=False)
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (final_proj): Linear(in_features=1024, out_features=768, bias=True)
)
Layer name: feature_extractor, Module: ConvFeatureExtractionModel(
  (conv_layers): ModuleList(
    (0): Sequential(
      (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,))
      (1): Dropout(p=0.0, inplace=False)
      (2): Sequential(
        (0): TransposeLast()
        (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (2): TransposeLast()
      )
      (3): GELU(approximate=none)
    )
    (1): Sequential(
      (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,))
      (1): Dropout(p=0.0, inplace=False)
      (2): Sequential(
        (0): TransposeLast()
        (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (2): TransposeLast()
      )
      (3): GELU(approximate=none)
    )
    (2): Sequential(
      (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,))
      (1): Dropout(p=0.0, inplace=False)
      (2): Sequential(
        (0): TransposeLast()
        (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (2): TransposeLast()
      )
      (3): GELU(approximate=none)
    )
    (3): Sequential(
      (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,))
      (1): Dropout(p=0.0, inplace=False)
      (2): Sequential(
        (0): TransposeLast()
        (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (2): TransposeLast()
      )
      (3): GELU(approximate=none)
    )
    (4): Sequential(
      (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,))
      (1): Dropout(p=0.0, inplace=False)
      (2): Sequential(
        (0): TransposeLast()
        (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (2): TransposeLast()
      )
      (3): GELU(approximate=none)
    )
    (5): Sequential(
      (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,))
      (1): Dropout(p=0.0, inplace=False)
      (2): Sequential(
        (0): TransposeLast()
        (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (2): TransposeLast()
      )
      (3): GELU(approximate=none)
    )
    (6): Sequential(
      (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,))
      (1): Dropout(p=0.0, inplace=False)
      (2): Sequential(
        (0): TransposeLast()
        (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (2): TransposeLast()
      )
      (3): GELU(approximate=none)
    )
  )
)
Layer name: feature_extractor.conv_layers, Module: ModuleList(
  (0): Sequential(
    (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,))
    (1): Dropout(p=0.0, inplace=False)
    (2): Sequential(
      (0): TransposeLast()
      (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (2): TransposeLast()
    )
    (3): GELU(approximate=none)
  )
  (1): Sequential(
    (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,))
    (1): Dropout(p=0.0, inplace=False)
    (2): Sequential(
      (0): TransposeLast()
      (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (2): TransposeLast()
    )
    (3): GELU(approximate=none)
  )
  (2): Sequential(
    (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,))
    (1): Dropout(p=0.0, inplace=False)
    (2): Sequential(
      (0): TransposeLast()
      (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (2): TransposeLast()
    )
    (3): GELU(approximate=none)
  )
  (3): Sequential(
    (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,))
    (1): Dropout(p=0.0, inplace=False)
    (2): Sequential(
      (0): TransposeLast()
      (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (2): TransposeLast()
    )
    (3): GELU(approximate=none)
  )
  (4): Sequential(
    (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,))
    (1): Dropout(p=0.0, inplace=False)
    (2): Sequential(
      (0): TransposeLast()
      (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (2): TransposeLast()
    )
    (3): GELU(approximate=none)
  )
  (5): Sequential(
    (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,))
    (1): Dropout(p=0.0, inplace=False)
    (2): Sequential(
      (0): TransposeLast()
      (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (2): TransposeLast()
    )
    (3): GELU(approximate=none)
  )
  (6): Sequential(
    (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,))
    (1): Dropout(p=0.0, inplace=False)
    (2): Sequential(
      (0): TransposeLast()
      (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (2): TransposeLast()
    )
    (3): GELU(approximate=none)
  )
)
Layer name: feature_extractor.conv_layers.0, Module: Sequential(
  (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,))
  (1): Dropout(p=0.0, inplace=False)
  (2): Sequential(
    (0): TransposeLast()
    (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (2): TransposeLast()
  )
  (3): GELU(approximate=none)
)
Layer name: feature_extractor.conv_layers.0.0, Module: Conv1d(1, 512, kernel_size=(10,), stride=(5,))
Layer name: feature_extractor.conv_layers.0.1, Module: Dropout(p=0.0, inplace=False)
Layer name: feature_extractor.conv_layers.0.2, Module: Sequential(
  (0): TransposeLast()
  (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (2): TransposeLast()
)
Layer name: feature_extractor.conv_layers.0.2.0, Module: TransposeLast()
Layer name: feature_extractor.conv_layers.0.2.1, Module: Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
Layer name: feature_extractor.conv_layers.0.2.2, Module: TransposeLast()
Layer name: feature_extractor.conv_layers.0.3, Module: GELU(approximate=none)
Layer name: feature_extractor.conv_layers.1, Module: Sequential(
  (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,))
  (1): Dropout(p=0.0, inplace=False)
  (2): Sequential(
    (0): TransposeLast()
    (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (2): TransposeLast()
  )
  (3): GELU(approximate=none)
)
Layer name: feature_extractor.conv_layers.1.0, Module: Conv1d(512, 512, kernel_size=(3,), stride=(2,))
Layer name: feature_extractor.conv_layers.1.1, Module: Dropout(p=0.0, inplace=False)
Layer name: feature_extractor.conv_layers.1.2, Module: Sequential(
  (0): TransposeLast()
  (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (2): TransposeLast()
)
Layer name: feature_extractor.conv_layers.1.2.0, Module: TransposeLast()
Layer name: feature_extractor.conv_layers.1.2.1, Module: Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
Layer name: feature_extractor.conv_layers.1.2.2, Module: TransposeLast()
Layer name: feature_extractor.conv_layers.1.3, Module: GELU(approximate=none)
Layer name: feature_extractor.conv_layers.2, Module: Sequential(
  (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,))
  (1): Dropout(p=0.0, inplace=False)
  (2): Sequential(
    (0): TransposeLast()
    (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (2): TransposeLast()
  )
  (3): GELU(approximate=none)
)
Layer name: feature_extractor.conv_layers.2.0, Module: Conv1d(512, 512, kernel_size=(3,), stride=(2,))
Layer name: feature_extractor.conv_layers.2.1, Module: Dropout(p=0.0, inplace=False)
Layer name: feature_extractor.conv_layers.2.2, Module: Sequential(
  (0): TransposeLast()
  (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (2): TransposeLast()
)
Layer name: feature_extractor.conv_layers.2.2.0, Module: TransposeLast()
Layer name: feature_extractor.conv_layers.2.2.1, Module: Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
Layer name: feature_extractor.conv_layers.2.2.2, Module: TransposeLast()
Layer name: feature_extractor.conv_layers.2.3, Module: GELU(approximate=none)
Layer name: feature_extractor.conv_layers.3, Module: Sequential(
  (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,))
  (1): Dropout(p=0.0, inplace=False)
  (2): Sequential(
    (0): TransposeLast()
    (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (2): TransposeLast()
  )
  (3): GELU(approximate=none)
)
Layer name: feature_extractor.conv_layers.3.0, Module: Conv1d(512, 512, kernel_size=(3,), stride=(2,))
Layer name: feature_extractor.conv_layers.3.1, Module: Dropout(p=0.0, inplace=False)
Layer name: feature_extractor.conv_layers.3.2, Module: Sequential(
  (0): TransposeLast()
  (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (2): TransposeLast()
)
Layer name: feature_extractor.conv_layers.3.2.0, Module: TransposeLast()
Layer name: feature_extractor.conv_layers.3.2.1, Module: Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
Layer name: feature_extractor.conv_layers.3.2.2, Module: TransposeLast()
Layer name: feature_extractor.conv_layers.3.3, Module: GELU(approximate=none)
Layer name: feature_extractor.conv_layers.4, Module: Sequential(
  (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,))
  (1): Dropout(p=0.0, inplace=False)
  (2): Sequential(
    (0): TransposeLast()
    (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (2): TransposeLast()
  )
  (3): GELU(approximate=none)
)
Layer name: feature_extractor.conv_layers.4.0, Module: Conv1d(512, 512, kernel_size=(3,), stride=(2,))
Layer name: feature_extractor.conv_layers.4.1, Module: Dropout(p=0.0, inplace=False)
Layer name: feature_extractor.conv_layers.4.2, Module: Sequential(
  (0): TransposeLast()
  (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (2): TransposeLast()
)
Layer name: feature_extractor.conv_layers.4.2.0, Module: TransposeLast()
Layer name: feature_extractor.conv_layers.4.2.1, Module: Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
Layer name: feature_extractor.conv_layers.4.2.2, Module: TransposeLast()
Layer name: feature_extractor.conv_layers.4.3, Module: GELU(approximate=none)
Layer name: feature_extractor.conv_layers.5, Module: Sequential(
  (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,))
  (1): Dropout(p=0.0, inplace=False)
  (2): Sequential(
    (0): TransposeLast()
    (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (2): TransposeLast()
  )
  (3): GELU(approximate=none)
)
Layer name: feature_extractor.conv_layers.5.0, Module: Conv1d(512, 512, kernel_size=(2,), stride=(2,))
Layer name: feature_extractor.conv_layers.5.1, Module: Dropout(p=0.0, inplace=False)
Layer name: feature_extractor.conv_layers.5.2, Module: Sequential(
  (0): TransposeLast()
  (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (2): TransposeLast()
)
Layer name: feature_extractor.conv_layers.5.2.0, Module: TransposeLast()
Layer name: feature_extractor.conv_layers.5.2.1, Module: Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
Layer name: feature_extractor.conv_layers.5.2.2, Module: TransposeLast()
Layer name: feature_extractor.conv_layers.5.3, Module: GELU(approximate=none)
Layer name: feature_extractor.conv_layers.6, Module: Sequential(
  (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,))
  (1): Dropout(p=0.0, inplace=False)
  (2): Sequential(
    (0): TransposeLast()
    (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (2): TransposeLast()
  )
  (3): GELU(approximate=none)
)
Layer name: feature_extractor.conv_layers.6.0, Module: Conv1d(512, 512, kernel_size=(2,), stride=(2,))
Layer name: feature_extractor.conv_layers.6.1, Module: Dropout(p=0.0, inplace=False)
Layer name: feature_extractor.conv_layers.6.2, Module: Sequential(
  (0): TransposeLast()
  (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (2): TransposeLast()
)
Layer name: feature_extractor.conv_layers.6.2.0, Module: TransposeLast()
Layer name: feature_extractor.conv_layers.6.2.1, Module: Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
Layer name: feature_extractor.conv_layers.6.2.2, Module: TransposeLast()
Layer name: feature_extractor.conv_layers.6.3, Module: GELU(approximate=none)
Layer name: post_extract_proj, Module: Linear(in_features=512, out_features=1024, bias=True)
Layer name: dropout_input, Module: Dropout(p=0.0, inplace=False)
Layer name: dropout_features, Module: Dropout(p=0.0, inplace=False)
Layer name: quantizer, Module: GumbelVectorQuantizer(
  (weight_proj): Linear(in_features=512, out_features=640, bias=True)
)
Layer name: quantizer.weight_proj, Module: Linear(in_features=512, out_features=640, bias=True)
Layer name: project_q, Module: Linear(in_features=768, out_features=768, bias=True)
Layer name: encoder, Module: TransformerEncoder(
  (pos_conv): Sequential(
    (0): Conv1d(1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
    (1): SamePad()
    (2): GELU(approximate=none)
  )
  (layers): ModuleList(
    (0): TransformerSentenceEncoderLayer(
      (self_attn): MultiheadAttention(
        (dropout_module): FairseqDropout()
        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
      )
      (dropout1): Dropout(p=0.0, inplace=False)
      (dropout2): Dropout(p=0.0, inplace=False)
      (dropout3): Dropout(p=0.0, inplace=False)
      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
    (1): TransformerSentenceEncoderLayer(
      (self_attn): MultiheadAttention(
        (dropout_module): FairseqDropout()
        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
      )
      (dropout1): Dropout(p=0.0, inplace=False)
      (dropout2): Dropout(p=0.0, inplace=False)
      (dropout3): Dropout(p=0.0, inplace=False)
      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
    (2): TransformerSentenceEncoderLayer(
      (self_attn): MultiheadAttention(
        (dropout_module): FairseqDropout()
        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
      )
      (dropout1): Dropout(p=0.0, inplace=False)
      (dropout2): Dropout(p=0.0, inplace=False)
      (dropout3): Dropout(p=0.0, inplace=False)
      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
    (3): TransformerSentenceEncoderLayer(
      (self_attn): MultiheadAttention(
        (dropout_module): FairseqDropout()
        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
      )
      (dropout1): Dropout(p=0.0, inplace=False)
      (dropout2): Dropout(p=0.0, inplace=False)
      (dropout3): Dropout(p=0.0, inplace=False)
      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
    (4): TransformerSentenceEncoderLayer(
      (self_attn): MultiheadAttention(
        (dropout_module): FairseqDropout()
        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
      )
      (dropout1): Dropout(p=0.0, inplace=False)
      (dropout2): Dropout(p=0.0, inplace=False)
      (dropout3): Dropout(p=0.0, inplace=False)
      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
    (5): TransformerSentenceEncoderLayer(
      (self_attn): MultiheadAttention(
        (dropout_module): FairseqDropout()
        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
      )
      (dropout1): Dropout(p=0.0, inplace=False)
      (dropout2): Dropout(p=0.0, inplace=False)
      (dropout3): Dropout(p=0.0, inplace=False)
      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
    (6): TransformerSentenceEncoderLayer(
      (self_attn): MultiheadAttention(
        (dropout_module): FairseqDropout()
        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
      )
      (dropout1): Dropout(p=0.0, inplace=False)
      (dropout2): Dropout(p=0.0, inplace=False)
      (dropout3): Dropout(p=0.0, inplace=False)
      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
    (7): TransformerSentenceEncoderLayer(
      (self_attn): MultiheadAttention(
        (dropout_module): FairseqDropout()
        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
      )
      (dropout1): Dropout(p=0.0, inplace=False)
      (dropout2): Dropout(p=0.0, inplace=False)
      (dropout3): Dropout(p=0.0, inplace=False)
      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
    (8): TransformerSentenceEncoderLayer(
      (self_attn): MultiheadAttention(
        (dropout_module): FairseqDropout()
        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
      )
      (dropout1): Dropout(p=0.0, inplace=False)
      (dropout2): Dropout(p=0.0, inplace=False)
      (dropout3): Dropout(p=0.0, inplace=False)
      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
    (9): TransformerSentenceEncoderLayer(
      (self_attn): MultiheadAttention(
        (dropout_module): FairseqDropout()
        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
      )
      (dropout1): Dropout(p=0.0, inplace=False)
      (dropout2): Dropout(p=0.0, inplace=False)
      (dropout3): Dropout(p=0.0, inplace=False)
      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
    (10): TransformerSentenceEncoderLayer(
      (self_attn): MultiheadAttention(
        (dropout_module): FairseqDropout()
        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
      )
      (dropout1): Dropout(p=0.0, inplace=False)
      (dropout2): Dropout(p=0.0, inplace=False)
      (dropout3): Dropout(p=0.0, inplace=False)
      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
    (11): TransformerSentenceEncoderLayer(
      (self_attn): MultiheadAttention(
        (dropout_module): FairseqDropout()
        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
      )
      (dropout1): Dropout(p=0.0, inplace=False)
      (dropout2): Dropout(p=0.0, inplace=False)
      (dropout3): Dropout(p=0.0, inplace=False)
      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
    (12): TransformerSentenceEncoderLayer(
      (self_attn): MultiheadAttention(
        (dropout_module): FairseqDropout()
        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
      )
      (dropout1): Dropout(p=0.0, inplace=False)
      (dropout2): Dropout(p=0.0, inplace=False)
      (dropout3): Dropout(p=0.0, inplace=False)
      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
    (13): TransformerSentenceEncoderLayer(
      (self_attn): MultiheadAttention(
        (dropout_module): FairseqDropout()
        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
      )
      (dropout1): Dropout(p=0.0, inplace=False)
      (dropout2): Dropout(p=0.0, inplace=False)
      (dropout3): Dropout(p=0.0, inplace=False)
      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
    (14): TransformerSentenceEncoderLayer(
      (self_attn): MultiheadAttention(
        (dropout_module): FairseqDropout()
        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
      )
      (dropout1): Dropout(p=0.0, inplace=False)
      (dropout2): Dropout(p=0.0, inplace=False)
      (dropout3): Dropout(p=0.0, inplace=False)
      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
    (15): TransformerSentenceEncoderLayer(
      (self_attn): MultiheadAttention(
        (dropout_module): FairseqDropout()
        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
      )
      (dropout1): Dropout(p=0.0, inplace=False)
      (dropout2): Dropout(p=0.0, inplace=False)
      (dropout3): Dropout(p=0.0, inplace=False)
      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
    (16): TransformerSentenceEncoderLayer(
      (self_attn): MultiheadAttention(
        (dropout_module): FairseqDropout()
        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
      )
      (dropout1): Dropout(p=0.0, inplace=False)
      (dropout2): Dropout(p=0.0, inplace=False)
      (dropout3): Dropout(p=0.0, inplace=False)
      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
    (17): TransformerSentenceEncoderLayer(
      (self_attn): MultiheadAttention(
        (dropout_module): FairseqDropout()
        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
      )
      (dropout1): Dropout(p=0.0, inplace=False)
      (dropout2): Dropout(p=0.0, inplace=False)
      (dropout3): Dropout(p=0.0, inplace=False)
      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
    (18): TransformerSentenceEncoderLayer(
      (self_attn): MultiheadAttention(
        (dropout_module): FairseqDropout()
        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
      )
      (dropout1): Dropout(p=0.0, inplace=False)
      (dropout2): Dropout(p=0.0, inplace=False)
      (dropout3): Dropout(p=0.0, inplace=False)
      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
    (19): TransformerSentenceEncoderLayer(
      (self_attn): MultiheadAttention(
        (dropout_module): FairseqDropout()
        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
      )
      (dropout1): Dropout(p=0.0, inplace=False)
      (dropout2): Dropout(p=0.0, inplace=False)
      (dropout3): Dropout(p=0.0, inplace=False)
      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
    (20): TransformerSentenceEncoderLayer(
      (self_attn): MultiheadAttention(
        (dropout_module): FairseqDropout()
        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
      )
      (dropout1): Dropout(p=0.0, inplace=False)
      (dropout2): Dropout(p=0.0, inplace=False)
      (dropout3): Dropout(p=0.0, inplace=False)
      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
    (21): TransformerSentenceEncoderLayer(
      (self_attn): MultiheadAttention(
        (dropout_module): FairseqDropout()
        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
      )
      (dropout1): Dropout(p=0.0, inplace=False)
      (dropout2): Dropout(p=0.0, inplace=False)
      (dropout3): Dropout(p=0.0, inplace=False)
      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
    (22): TransformerSentenceEncoderLayer(
      (self_attn): MultiheadAttention(
        (dropout_module): FairseqDropout()
        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
      )
      (dropout1): Dropout(p=0.0, inplace=False)
      (dropout2): Dropout(p=0.0, inplace=False)
      (dropout3): Dropout(p=0.0, inplace=False)
      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
    (23): TransformerSentenceEncoderLayer(
      (self_attn): MultiheadAttention(
        (dropout_module): FairseqDropout()
        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
      )
      (dropout1): Dropout(p=0.0, inplace=False)
      (dropout2): Dropout(p=0.0, inplace=False)
      (dropout3): Dropout(p=0.0, inplace=False)
      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (fc1): Linear(in_features=1024, out_features=4096, bias=True)
      (fc2): Linear(in_features=4096, out_features=1024, bias=True)
      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
  (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
)
Layer name: encoder.pos_conv, Module: Sequential(
  (0): Conv1d(1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
  (1): SamePad()
  (2): GELU(approximate=none)
)
Layer name: encoder.pos_conv.0, Module: Conv1d(1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
Layer name: encoder.pos_conv.1, Module: SamePad()
Layer name: encoder.pos_conv.2, Module: GELU(approximate=none)
Layer name: encoder.layers, Module: ModuleList(
  (0): TransformerSentenceEncoderLayer(
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (dropout1): Dropout(p=0.0, inplace=False)
    (dropout2): Dropout(p=0.0, inplace=False)
    (dropout3): Dropout(p=0.0, inplace=False)
    (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (1): TransformerSentenceEncoderLayer(
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (dropout1): Dropout(p=0.0, inplace=False)
    (dropout2): Dropout(p=0.0, inplace=False)
    (dropout3): Dropout(p=0.0, inplace=False)
    (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (2): TransformerSentenceEncoderLayer(
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (dropout1): Dropout(p=0.0, inplace=False)
    (dropout2): Dropout(p=0.0, inplace=False)
    (dropout3): Dropout(p=0.0, inplace=False)
    (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (3): TransformerSentenceEncoderLayer(
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (dropout1): Dropout(p=0.0, inplace=False)
    (dropout2): Dropout(p=0.0, inplace=False)
    (dropout3): Dropout(p=0.0, inplace=False)
    (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (4): TransformerSentenceEncoderLayer(
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (dropout1): Dropout(p=0.0, inplace=False)
    (dropout2): Dropout(p=0.0, inplace=False)
    (dropout3): Dropout(p=0.0, inplace=False)
    (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (5): TransformerSentenceEncoderLayer(
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (dropout1): Dropout(p=0.0, inplace=False)
    (dropout2): Dropout(p=0.0, inplace=False)
    (dropout3): Dropout(p=0.0, inplace=False)
    (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (6): TransformerSentenceEncoderLayer(
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (dropout1): Dropout(p=0.0, inplace=False)
    (dropout2): Dropout(p=0.0, inplace=False)
    (dropout3): Dropout(p=0.0, inplace=False)
    (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (7): TransformerSentenceEncoderLayer(
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (dropout1): Dropout(p=0.0, inplace=False)
    (dropout2): Dropout(p=0.0, inplace=False)
    (dropout3): Dropout(p=0.0, inplace=False)
    (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (8): TransformerSentenceEncoderLayer(
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (dropout1): Dropout(p=0.0, inplace=False)
    (dropout2): Dropout(p=0.0, inplace=False)
    (dropout3): Dropout(p=0.0, inplace=False)
    (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (9): TransformerSentenceEncoderLayer(
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (dropout1): Dropout(p=0.0, inplace=False)
    (dropout2): Dropout(p=0.0, inplace=False)
    (dropout3): Dropout(p=0.0, inplace=False)
    (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (10): TransformerSentenceEncoderLayer(
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (dropout1): Dropout(p=0.0, inplace=False)
    (dropout2): Dropout(p=0.0, inplace=False)
    (dropout3): Dropout(p=0.0, inplace=False)
    (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (11): TransformerSentenceEncoderLayer(
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (dropout1): Dropout(p=0.0, inplace=False)
    (dropout2): Dropout(p=0.0, inplace=False)
    (dropout3): Dropout(p=0.0, inplace=False)
    (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (12): TransformerSentenceEncoderLayer(
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (dropout1): Dropout(p=0.0, inplace=False)
    (dropout2): Dropout(p=0.0, inplace=False)
    (dropout3): Dropout(p=0.0, inplace=False)
    (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (13): TransformerSentenceEncoderLayer(
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (dropout1): Dropout(p=0.0, inplace=False)
    (dropout2): Dropout(p=0.0, inplace=False)
    (dropout3): Dropout(p=0.0, inplace=False)
    (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (14): TransformerSentenceEncoderLayer(
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (dropout1): Dropout(p=0.0, inplace=False)
    (dropout2): Dropout(p=0.0, inplace=False)
    (dropout3): Dropout(p=0.0, inplace=False)
    (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (15): TransformerSentenceEncoderLayer(
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (dropout1): Dropout(p=0.0, inplace=False)
    (dropout2): Dropout(p=0.0, inplace=False)
    (dropout3): Dropout(p=0.0, inplace=False)
    (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (16): TransformerSentenceEncoderLayer(
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (dropout1): Dropout(p=0.0, inplace=False)
    (dropout2): Dropout(p=0.0, inplace=False)
    (dropout3): Dropout(p=0.0, inplace=False)
    (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (17): TransformerSentenceEncoderLayer(
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (dropout1): Dropout(p=0.0, inplace=False)
    (dropout2): Dropout(p=0.0, inplace=False)
    (dropout3): Dropout(p=0.0, inplace=False)
    (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (18): TransformerSentenceEncoderLayer(
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (dropout1): Dropout(p=0.0, inplace=False)
    (dropout2): Dropout(p=0.0, inplace=False)
    (dropout3): Dropout(p=0.0, inplace=False)
    (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (19): TransformerSentenceEncoderLayer(
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (dropout1): Dropout(p=0.0, inplace=False)
    (dropout2): Dropout(p=0.0, inplace=False)
    (dropout3): Dropout(p=0.0, inplace=False)
    (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (20): TransformerSentenceEncoderLayer(
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (dropout1): Dropout(p=0.0, inplace=False)
    (dropout2): Dropout(p=0.0, inplace=False)
    (dropout3): Dropout(p=0.0, inplace=False)
    (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (21): TransformerSentenceEncoderLayer(
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (dropout1): Dropout(p=0.0, inplace=False)
    (dropout2): Dropout(p=0.0, inplace=False)
    (dropout3): Dropout(p=0.0, inplace=False)
    (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (22): TransformerSentenceEncoderLayer(
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (dropout1): Dropout(p=0.0, inplace=False)
    (dropout2): Dropout(p=0.0, inplace=False)
    (dropout3): Dropout(p=0.0, inplace=False)
    (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (23): TransformerSentenceEncoderLayer(
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (dropout1): Dropout(p=0.0, inplace=False)
    (dropout2): Dropout(p=0.0, inplace=False)
    (dropout3): Dropout(p=0.0, inplace=False)
    (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
)
Layer name: encoder.layers.0, Module: TransformerSentenceEncoderLayer(
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
  )
  (dropout1): Dropout(p=0.0, inplace=False)
  (dropout2): Dropout(p=0.0, inplace=False)
  (dropout3): Dropout(p=0.0, inplace=False)
  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
)
Layer name: encoder.layers.0.self_attn, Module: MultiheadAttention(
  (dropout_module): FairseqDropout()
  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
)
Layer name: encoder.layers.0.self_attn.dropout_module, Module: FairseqDropout()
Layer name: encoder.layers.0.self_attn.k_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.0.self_attn.v_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.0.self_attn.q_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.0.self_attn.out_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.0.dropout1, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.0.dropout2, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.0.dropout3, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.0.self_attn_layer_norm, Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
Layer name: encoder.layers.0.fc1, Module: Linear(in_features=1024, out_features=4096, bias=True)
Layer name: encoder.layers.0.fc2, Module: Linear(in_features=4096, out_features=1024, bias=True)
Layer name: encoder.layers.0.final_layer_norm, Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
Layer name: encoder.layers.1, Module: TransformerSentenceEncoderLayer(
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
  )
  (dropout1): Dropout(p=0.0, inplace=False)
  (dropout2): Dropout(p=0.0, inplace=False)
  (dropout3): Dropout(p=0.0, inplace=False)
  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
)
Layer name: encoder.layers.1.self_attn, Module: MultiheadAttention(
  (dropout_module): FairseqDropout()
  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
)
Layer name: encoder.layers.1.self_attn.dropout_module, Module: FairseqDropout()
Layer name: encoder.layers.1.self_attn.k_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.1.self_attn.v_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.1.self_attn.q_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.1.self_attn.out_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.1.dropout1, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.1.dropout2, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.1.dropout3, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.1.self_attn_layer_norm, Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
Layer name: encoder.layers.1.fc1, Module: Linear(in_features=1024, out_features=4096, bias=True)
Layer name: encoder.layers.1.fc2, Module: Linear(in_features=4096, out_features=1024, bias=True)
Layer name: encoder.layers.1.final_layer_norm, Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
Layer name: encoder.layers.2, Module: TransformerSentenceEncoderLayer(
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
  )
  (dropout1): Dropout(p=0.0, inplace=False)
  (dropout2): Dropout(p=0.0, inplace=False)
  (dropout3): Dropout(p=0.0, inplace=False)
  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
)
Layer name: encoder.layers.2.self_attn, Module: MultiheadAttention(
  (dropout_module): FairseqDropout()
  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
)
Layer name: encoder.layers.2.self_attn.dropout_module, Module: FairseqDropout()
Layer name: encoder.layers.2.self_attn.k_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.2.self_attn.v_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.2.self_attn.q_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.2.self_attn.out_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.2.dropout1, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.2.dropout2, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.2.dropout3, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.2.self_attn_layer_norm, Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
Layer name: encoder.layers.2.fc1, Module: Linear(in_features=1024, out_features=4096, bias=True)
Layer name: encoder.layers.2.fc2, Module: Linear(in_features=4096, out_features=1024, bias=True)
Layer name: encoder.layers.2.final_layer_norm, Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
Layer name: encoder.layers.3, Module: TransformerSentenceEncoderLayer(
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
  )
  (dropout1): Dropout(p=0.0, inplace=False)
  (dropout2): Dropout(p=0.0, inplace=False)
  (dropout3): Dropout(p=0.0, inplace=False)
  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
)
Layer name: encoder.layers.3.self_attn, Module: MultiheadAttention(
  (dropout_module): FairseqDropout()
  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
)
Layer name: encoder.layers.3.self_attn.dropout_module, Module: FairseqDropout()
Layer name: encoder.layers.3.self_attn.k_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.3.self_attn.v_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.3.self_attn.q_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.3.self_attn.out_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.3.dropout1, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.3.dropout2, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.3.dropout3, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.3.self_attn_layer_norm, Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
Layer name: encoder.layers.3.fc1, Module: Linear(in_features=1024, out_features=4096, bias=True)
Layer name: encoder.layers.3.fc2, Module: Linear(in_features=4096, out_features=1024, bias=True)
Layer name: encoder.layers.3.final_layer_norm, Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
Layer name: encoder.layers.4, Module: TransformerSentenceEncoderLayer(
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
  )
  (dropout1): Dropout(p=0.0, inplace=False)
  (dropout2): Dropout(p=0.0, inplace=False)
  (dropout3): Dropout(p=0.0, inplace=False)
  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
)
Layer name: encoder.layers.4.self_attn, Module: MultiheadAttention(
  (dropout_module): FairseqDropout()
  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
)
Layer name: encoder.layers.4.self_attn.dropout_module, Module: FairseqDropout()
Layer name: encoder.layers.4.self_attn.k_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.4.self_attn.v_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.4.self_attn.q_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.4.self_attn.out_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.4.dropout1, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.4.dropout2, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.4.dropout3, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.4.self_attn_layer_norm, Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
Layer name: encoder.layers.4.fc1, Module: Linear(in_features=1024, out_features=4096, bias=True)
Layer name: encoder.layers.4.fc2, Module: Linear(in_features=4096, out_features=1024, bias=True)
Layer name: encoder.layers.4.final_layer_norm, Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
Layer name: encoder.layers.5, Module: TransformerSentenceEncoderLayer(
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
  )
  (dropout1): Dropout(p=0.0, inplace=False)
  (dropout2): Dropout(p=0.0, inplace=False)
  (dropout3): Dropout(p=0.0, inplace=False)
  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
)
Layer name: encoder.layers.5.self_attn, Module: MultiheadAttention(
  (dropout_module): FairseqDropout()
  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
)
Layer name: encoder.layers.5.self_attn.dropout_module, Module: FairseqDropout()
Layer name: encoder.layers.5.self_attn.k_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.5.self_attn.v_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.5.self_attn.q_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.5.self_attn.out_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.5.dropout1, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.5.dropout2, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.5.dropout3, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.5.self_attn_layer_norm, Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
Layer name: encoder.layers.5.fc1, Module: Linear(in_features=1024, out_features=4096, bias=True)
Layer name: encoder.layers.5.fc2, Module: Linear(in_features=4096, out_features=1024, bias=True)
Layer name: encoder.layers.5.final_layer_norm, Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
Layer name: encoder.layers.6, Module: TransformerSentenceEncoderLayer(
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
  )
  (dropout1): Dropout(p=0.0, inplace=False)
  (dropout2): Dropout(p=0.0, inplace=False)
  (dropout3): Dropout(p=0.0, inplace=False)
  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
)
Layer name: encoder.layers.6.self_attn, Module: MultiheadAttention(
  (dropout_module): FairseqDropout()
  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
)
Layer name: encoder.layers.6.self_attn.dropout_module, Module: FairseqDropout()
Layer name: encoder.layers.6.self_attn.k_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.6.self_attn.v_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.6.self_attn.q_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.6.self_attn.out_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.6.dropout1, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.6.dropout2, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.6.dropout3, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.6.self_attn_layer_norm, Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
Layer name: encoder.layers.6.fc1, Module: Linear(in_features=1024, out_features=4096, bias=True)
Layer name: encoder.layers.6.fc2, Module: Linear(in_features=4096, out_features=1024, bias=True)
Layer name: encoder.layers.6.final_layer_norm, Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
Layer name: encoder.layers.7, Module: TransformerSentenceEncoderLayer(
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
  )
  (dropout1): Dropout(p=0.0, inplace=False)
  (dropout2): Dropout(p=0.0, inplace=False)
  (dropout3): Dropout(p=0.0, inplace=False)
  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
)
Layer name: encoder.layers.7.self_attn, Module: MultiheadAttention(
  (dropout_module): FairseqDropout()
  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
)
Layer name: encoder.layers.7.self_attn.dropout_module, Module: FairseqDropout()
Layer name: encoder.layers.7.self_attn.k_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.7.self_attn.v_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.7.self_attn.q_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.7.self_attn.out_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.7.dropout1, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.7.dropout2, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.7.dropout3, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.7.self_attn_layer_norm, Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
Layer name: encoder.layers.7.fc1, Module: Linear(in_features=1024, out_features=4096, bias=True)
Layer name: encoder.layers.7.fc2, Module: Linear(in_features=4096, out_features=1024, bias=True)
Layer name: encoder.layers.7.final_layer_norm, Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
Layer name: encoder.layers.8, Module: TransformerSentenceEncoderLayer(
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
  )
  (dropout1): Dropout(p=0.0, inplace=False)
  (dropout2): Dropout(p=0.0, inplace=False)
  (dropout3): Dropout(p=0.0, inplace=False)
  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
)
Layer name: encoder.layers.8.self_attn, Module: MultiheadAttention(
  (dropout_module): FairseqDropout()
  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
)
Layer name: encoder.layers.8.self_attn.dropout_module, Module: FairseqDropout()
Layer name: encoder.layers.8.self_attn.k_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.8.self_attn.v_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.8.self_attn.q_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.8.self_attn.out_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.8.dropout1, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.8.dropout2, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.8.dropout3, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.8.self_attn_layer_norm, Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
Layer name: encoder.layers.8.fc1, Module: Linear(in_features=1024, out_features=4096, bias=True)
Layer name: encoder.layers.8.fc2, Module: Linear(in_features=4096, out_features=1024, bias=True)
Layer name: encoder.layers.8.final_layer_norm, Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
Layer name: encoder.layers.9, Module: TransformerSentenceEncoderLayer(
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
  )
  (dropout1): Dropout(p=0.0, inplace=False)
  (dropout2): Dropout(p=0.0, inplace=False)
  (dropout3): Dropout(p=0.0, inplace=False)
  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
)
Layer name: encoder.layers.9.self_attn, Module: MultiheadAttention(
  (dropout_module): FairseqDropout()
  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
)
Layer name: encoder.layers.9.self_attn.dropout_module, Module: FairseqDropout()
Layer name: encoder.layers.9.self_attn.k_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.9.self_attn.v_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.9.self_attn.q_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.9.self_attn.out_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.9.dropout1, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.9.dropout2, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.9.dropout3, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.9.self_attn_layer_norm, Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
Layer name: encoder.layers.9.fc1, Module: Linear(in_features=1024, out_features=4096, bias=True)
Layer name: encoder.layers.9.fc2, Module: Linear(in_features=4096, out_features=1024, bias=True)
Layer name: encoder.layers.9.final_layer_norm, Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
Layer name: encoder.layers.10, Module: TransformerSentenceEncoderLayer(
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
  )
  (dropout1): Dropout(p=0.0, inplace=False)
  (dropout2): Dropout(p=0.0, inplace=False)
  (dropout3): Dropout(p=0.0, inplace=False)
  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
)
Layer name: encoder.layers.10.self_attn, Module: MultiheadAttention(
  (dropout_module): FairseqDropout()
  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
)
Layer name: encoder.layers.10.self_attn.dropout_module, Module: FairseqDropout()
Layer name: encoder.layers.10.self_attn.k_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.10.self_attn.v_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.10.self_attn.q_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.10.self_attn.out_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.10.dropout1, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.10.dropout2, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.10.dropout3, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.10.self_attn_layer_norm, Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
Layer name: encoder.layers.10.fc1, Module: Linear(in_features=1024, out_features=4096, bias=True)
Layer name: encoder.layers.10.fc2, Module: Linear(in_features=4096, out_features=1024, bias=True)
Layer name: encoder.layers.10.final_layer_norm, Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
Layer name: encoder.layers.11, Module: TransformerSentenceEncoderLayer(
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
  )
  (dropout1): Dropout(p=0.0, inplace=False)
  (dropout2): Dropout(p=0.0, inplace=False)
  (dropout3): Dropout(p=0.0, inplace=False)
  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
)
Layer name: encoder.layers.11.self_attn, Module: MultiheadAttention(
  (dropout_module): FairseqDropout()
  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
)
Layer name: encoder.layers.11.self_attn.dropout_module, Module: FairseqDropout()
Layer name: encoder.layers.11.self_attn.k_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.11.self_attn.v_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.11.self_attn.q_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.11.self_attn.out_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.11.dropout1, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.11.dropout2, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.11.dropout3, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.11.self_attn_layer_norm, Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
Layer name: encoder.layers.11.fc1, Module: Linear(in_features=1024, out_features=4096, bias=True)
Layer name: encoder.layers.11.fc2, Module: Linear(in_features=4096, out_features=1024, bias=True)
Layer name: encoder.layers.11.final_layer_norm, Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
Layer name: encoder.layers.12, Module: TransformerSentenceEncoderLayer(
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
  )
  (dropout1): Dropout(p=0.0, inplace=False)
  (dropout2): Dropout(p=0.0, inplace=False)
  (dropout3): Dropout(p=0.0, inplace=False)
  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
)
Layer name: encoder.layers.12.self_attn, Module: MultiheadAttention(
  (dropout_module): FairseqDropout()
  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
)
Layer name: encoder.layers.12.self_attn.dropout_module, Module: FairseqDropout()
Layer name: encoder.layers.12.self_attn.k_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.12.self_attn.v_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.12.self_attn.q_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.12.self_attn.out_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.12.dropout1, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.12.dropout2, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.12.dropout3, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.12.self_attn_layer_norm, Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
Layer name: encoder.layers.12.fc1, Module: Linear(in_features=1024, out_features=4096, bias=True)
Layer name: encoder.layers.12.fc2, Module: Linear(in_features=4096, out_features=1024, bias=True)
Layer name: encoder.layers.12.final_layer_norm, Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
Layer name: encoder.layers.13, Module: TransformerSentenceEncoderLayer(
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
  )
  (dropout1): Dropout(p=0.0, inplace=False)
  (dropout2): Dropout(p=0.0, inplace=False)
  (dropout3): Dropout(p=0.0, inplace=False)
  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
)
Layer name: encoder.layers.13.self_attn, Module: MultiheadAttention(
  (dropout_module): FairseqDropout()
  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
)
Layer name: encoder.layers.13.self_attn.dropout_module, Module: FairseqDropout()
Layer name: encoder.layers.13.self_attn.k_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.13.self_attn.v_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.13.self_attn.q_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.13.self_attn.out_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.13.dropout1, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.13.dropout2, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.13.dropout3, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.13.self_attn_layer_norm, Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
Layer name: encoder.layers.13.fc1, Module: Linear(in_features=1024, out_features=4096, bias=True)
Layer name: encoder.layers.13.fc2, Module: Linear(in_features=4096, out_features=1024, bias=True)
Layer name: encoder.layers.13.final_layer_norm, Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
Layer name: encoder.layers.14, Module: TransformerSentenceEncoderLayer(
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
  )
  (dropout1): Dropout(p=0.0, inplace=False)
  (dropout2): Dropout(p=0.0, inplace=False)
  (dropout3): Dropout(p=0.0, inplace=False)
  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
)
Layer name: encoder.layers.14.self_attn, Module: MultiheadAttention(
  (dropout_module): FairseqDropout()
  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
)
Layer name: encoder.layers.14.self_attn.dropout_module, Module: FairseqDropout()
Layer name: encoder.layers.14.self_attn.k_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.14.self_attn.v_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.14.self_attn.q_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.14.self_attn.out_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.14.dropout1, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.14.dropout2, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.14.dropout3, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.14.self_attn_layer_norm, Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
Layer name: encoder.layers.14.fc1, Module: Linear(in_features=1024, out_features=4096, bias=True)
Layer name: encoder.layers.14.fc2, Module: Linear(in_features=4096, out_features=1024, bias=True)
Layer name: encoder.layers.14.final_layer_norm, Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
Layer name: encoder.layers.15, Module: TransformerSentenceEncoderLayer(
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
  )
  (dropout1): Dropout(p=0.0, inplace=False)
  (dropout2): Dropout(p=0.0, inplace=False)
  (dropout3): Dropout(p=0.0, inplace=False)
  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
)
Layer name: encoder.layers.15.self_attn, Module: MultiheadAttention(
  (dropout_module): FairseqDropout()
  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
)
Layer name: encoder.layers.15.self_attn.dropout_module, Module: FairseqDropout()
Layer name: encoder.layers.15.self_attn.k_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.15.self_attn.v_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.15.self_attn.q_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.15.self_attn.out_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.15.dropout1, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.15.dropout2, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.15.dropout3, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.15.self_attn_layer_norm, Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
Layer name: encoder.layers.15.fc1, Module: Linear(in_features=1024, out_features=4096, bias=True)
Layer name: encoder.layers.15.fc2, Module: Linear(in_features=4096, out_features=1024, bias=True)
Layer name: encoder.layers.15.final_layer_norm, Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
Layer name: encoder.layers.16, Module: TransformerSentenceEncoderLayer(
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
  )
  (dropout1): Dropout(p=0.0, inplace=False)
  (dropout2): Dropout(p=0.0, inplace=False)
  (dropout3): Dropout(p=0.0, inplace=False)
  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
)
Layer name: encoder.layers.16.self_attn, Module: MultiheadAttention(
  (dropout_module): FairseqDropout()
  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
)
Layer name: encoder.layers.16.self_attn.dropout_module, Module: FairseqDropout()
Layer name: encoder.layers.16.self_attn.k_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.16.self_attn.v_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.16.self_attn.q_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.16.self_attn.out_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.16.dropout1, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.16.dropout2, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.16.dropout3, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.16.self_attn_layer_norm, Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
Layer name: encoder.layers.16.fc1, Module: Linear(in_features=1024, out_features=4096, bias=True)
Layer name: encoder.layers.16.fc2, Module: Linear(in_features=4096, out_features=1024, bias=True)
Layer name: encoder.layers.16.final_layer_norm, Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
Layer name: encoder.layers.17, Module: TransformerSentenceEncoderLayer(
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
  )
  (dropout1): Dropout(p=0.0, inplace=False)
  (dropout2): Dropout(p=0.0, inplace=False)
  (dropout3): Dropout(p=0.0, inplace=False)
  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
)
Layer name: encoder.layers.17.self_attn, Module: MultiheadAttention(
  (dropout_module): FairseqDropout()
  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
)
Layer name: encoder.layers.17.self_attn.dropout_module, Module: FairseqDropout()
Layer name: encoder.layers.17.self_attn.k_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.17.self_attn.v_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.17.self_attn.q_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.17.self_attn.out_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.17.dropout1, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.17.dropout2, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.17.dropout3, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.17.self_attn_layer_norm, Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
Layer name: encoder.layers.17.fc1, Module: Linear(in_features=1024, out_features=4096, bias=True)
Layer name: encoder.layers.17.fc2, Module: Linear(in_features=4096, out_features=1024, bias=True)
Layer name: encoder.layers.17.final_layer_norm, Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
Layer name: encoder.layers.18, Module: TransformerSentenceEncoderLayer(
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
  )
  (dropout1): Dropout(p=0.0, inplace=False)
  (dropout2): Dropout(p=0.0, inplace=False)
  (dropout3): Dropout(p=0.0, inplace=False)
  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
)
Layer name: encoder.layers.18.self_attn, Module: MultiheadAttention(
  (dropout_module): FairseqDropout()
  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
)
Layer name: encoder.layers.18.self_attn.dropout_module, Module: FairseqDropout()
Layer name: encoder.layers.18.self_attn.k_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.18.self_attn.v_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.18.self_attn.q_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.18.self_attn.out_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.18.dropout1, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.18.dropout2, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.18.dropout3, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.18.self_attn_layer_norm, Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
Layer name: encoder.layers.18.fc1, Module: Linear(in_features=1024, out_features=4096, bias=True)
Layer name: encoder.layers.18.fc2, Module: Linear(in_features=4096, out_features=1024, bias=True)
Layer name: encoder.layers.18.final_layer_norm, Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
Layer name: encoder.layers.19, Module: TransformerSentenceEncoderLayer(
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
  )
  (dropout1): Dropout(p=0.0, inplace=False)
  (dropout2): Dropout(p=0.0, inplace=False)
  (dropout3): Dropout(p=0.0, inplace=False)
  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
)
Layer name: encoder.layers.19.self_attn, Module: MultiheadAttention(
  (dropout_module): FairseqDropout()
  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
)
Layer name: encoder.layers.19.self_attn.dropout_module, Module: FairseqDropout()
Layer name: encoder.layers.19.self_attn.k_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.19.self_attn.v_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.19.self_attn.q_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.19.self_attn.out_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.19.dropout1, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.19.dropout2, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.19.dropout3, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.19.self_attn_layer_norm, Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
Layer name: encoder.layers.19.fc1, Module: Linear(in_features=1024, out_features=4096, bias=True)
Layer name: encoder.layers.19.fc2, Module: Linear(in_features=4096, out_features=1024, bias=True)
Layer name: encoder.layers.19.final_layer_norm, Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
Layer name: encoder.layers.20, Module: TransformerSentenceEncoderLayer(
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
  )
  (dropout1): Dropout(p=0.0, inplace=False)
  (dropout2): Dropout(p=0.0, inplace=False)
  (dropout3): Dropout(p=0.0, inplace=False)
  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
)
Layer name: encoder.layers.20.self_attn, Module: MultiheadAttention(
  (dropout_module): FairseqDropout()
  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
)
Layer name: encoder.layers.20.self_attn.dropout_module, Module: FairseqDropout()
Layer name: encoder.layers.20.self_attn.k_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.20.self_attn.v_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.20.self_attn.q_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.20.self_attn.out_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.20.dropout1, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.20.dropout2, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.20.dropout3, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.20.self_attn_layer_norm, Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
Layer name: encoder.layers.20.fc1, Module: Linear(in_features=1024, out_features=4096, bias=True)
Layer name: encoder.layers.20.fc2, Module: Linear(in_features=4096, out_features=1024, bias=True)
Layer name: encoder.layers.20.final_layer_norm, Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
Layer name: encoder.layers.21, Module: TransformerSentenceEncoderLayer(
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
  )
  (dropout1): Dropout(p=0.0, inplace=False)
  (dropout2): Dropout(p=0.0, inplace=False)
  (dropout3): Dropout(p=0.0, inplace=False)
  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
)
Layer name: encoder.layers.21.self_attn, Module: MultiheadAttention(
  (dropout_module): FairseqDropout()
  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
)
Layer name: encoder.layers.21.self_attn.dropout_module, Module: FairseqDropout()
Layer name: encoder.layers.21.self_attn.k_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.21.self_attn.v_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.21.self_attn.q_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.21.self_attn.out_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.21.dropout1, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.21.dropout2, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.21.dropout3, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.21.self_attn_layer_norm, Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
Layer name: encoder.layers.21.fc1, Module: Linear(in_features=1024, out_features=4096, bias=True)
Layer name: encoder.layers.21.fc2, Module: Linear(in_features=4096, out_features=1024, bias=True)
Layer name: encoder.layers.21.final_layer_norm, Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
Layer name: encoder.layers.22, Module: TransformerSentenceEncoderLayer(
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
  )
  (dropout1): Dropout(p=0.0, inplace=False)
  (dropout2): Dropout(p=0.0, inplace=False)
  (dropout3): Dropout(p=0.0, inplace=False)
  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
)
Layer name: encoder.layers.22.self_attn, Module: MultiheadAttention(
  (dropout_module): FairseqDropout()
  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
)
Layer name: encoder.layers.22.self_attn.dropout_module, Module: FairseqDropout()
Layer name: encoder.layers.22.self_attn.k_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.22.self_attn.v_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.22.self_attn.q_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.22.self_attn.out_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.22.dropout1, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.22.dropout2, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.22.dropout3, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.22.self_attn_layer_norm, Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
Layer name: encoder.layers.22.fc1, Module: Linear(in_features=1024, out_features=4096, bias=True)
Layer name: encoder.layers.22.fc2, Module: Linear(in_features=4096, out_features=1024, bias=True)
Layer name: encoder.layers.22.final_layer_norm, Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
Layer name: encoder.layers.23, Module: TransformerSentenceEncoderLayer(
  (self_attn): MultiheadAttention(
    (dropout_module): FairseqDropout()
    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
  )
  (dropout1): Dropout(p=0.0, inplace=False)
  (dropout2): Dropout(p=0.0, inplace=False)
  (dropout3): Dropout(p=0.0, inplace=False)
  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
)
Layer name: encoder.layers.23.self_attn, Module: MultiheadAttention(
  (dropout_module): FairseqDropout()
  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
)
Layer name: encoder.layers.23.self_attn.dropout_module, Module: FairseqDropout()
Layer name: encoder.layers.23.self_attn.k_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.23.self_attn.v_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.23.self_attn.q_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.23.self_attn.out_proj, Module: Linear(in_features=1024, out_features=1024, bias=True)
Layer name: encoder.layers.23.dropout1, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.23.dropout2, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.23.dropout3, Module: Dropout(p=0.0, inplace=False)
Layer name: encoder.layers.23.self_attn_layer_norm, Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
Layer name: encoder.layers.23.fc1, Module: Linear(in_features=1024, out_features=4096, bias=True)
Layer name: encoder.layers.23.fc2, Module: Linear(in_features=4096, out_features=1024, bias=True)
Layer name: encoder.layers.23.final_layer_norm, Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
Layer name: encoder.layer_norm, Module: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
Layer name: layer_norm, Module: LayerNorm((512,), eps=1e-05, elementwise_affine=True)
Layer name: final_proj, Module: Linear(in_features=1024, out_features=768, bias=True)
